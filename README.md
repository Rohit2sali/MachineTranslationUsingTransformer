This is repository is to solve the machine translation task using transformer. 
The Transformer is implemented using python and PyTorch.
This is the reimplementation of the paper "Attention is All You Need" .

Components of Transformer
Tokenizer
Embedding
Self-Attention
Masked Attention
Purpose
This repository is for learning and experiment purposes.
The code is written in a beginner-friendly way, making it easy to understand and modify.

How to RUn

Clone the Repository
git clone https://github.com/Rohit2sali/TransformerusingPyTorch

Training - To train the model and save it:
python train.py

Inference - For inferencing and testing with custom inputs:
python translate.py
