* This is the reimplementation of the paper Attention is all you need using pytorch . This repository contains the code of transformer architecture for machine translation on the language english to german.
- It contains all the components of Transformer as -
- Tokenizer
- Embedding
- Self-Attention
- MaskedAttention
  * This repository is for learning and experiment purpose the code written is very beginner friendly.
  If you have to run this on your local machine then first you have to download the repo using the link https://github.com/Rohit2sali/TransformerusingPyTorch and then you have to run the train.py file,
  which will train and save your model and then for inferencing you need to run translate.py file , in which you can give whatever input you want.
