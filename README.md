Transformer using PyTorch
This is the reimplementation of the paper "Attention is All You Need" using PyTorch.
This repository contains the code of Transformer architecture for machine translation on the language English to German.

Components of Transformer
Tokenizer
Embedding
Self-Attention
Masked Attention
Purpose
This repository is for learning and experiment purposes.
The code is written in a beginner-friendly way, making it easy to understand and modify.

How to RUn

Clone the Repository
git clone https://github.com/Rohit2sali/TransformerusingPyTorch

Training - To train the model and save it:
python train.py

Inference - For inferencing and testing with custom inputs:
python translate.py
